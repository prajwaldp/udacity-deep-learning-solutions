{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  \n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "  \n",
    "    statinfo = os.stat(filename)\n",
    "    \n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "      \n",
    "    return filename\n",
    "\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    \n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "    \n",
    "    return 0\n",
    "  \n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "    \n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "\n",
    "class BatchGenerator(object):\n",
    "\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        \n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        \n",
    "        return batch\n",
    "\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        \n",
    "        batches = [self._last_batch]\n",
    "        \n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "  \n",
    "        self._last_batch = batches[-1]\n",
    "        \n",
    "        return batches\n",
    "\n",
    "    \n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    \n",
    "    s = [''] * batches[0].shape[0]\n",
    "    \n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    \n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    \n",
    "    for i in range(len(distribution)):\n",
    "    \n",
    "        s += distribution[i]\n",
    "        \n",
    "        if s >= r:\n",
    "            return i\n",
    "    \n",
    "    return len(distribution) - 1\n",
    "\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b / np.sum(b, 1)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Memory cell: input, state and bias.\n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        \n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        \n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        \n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        \n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        \n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    \n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    \n",
    "    # labels are inputs shifted by one time step.\n",
    "    train_labels = train_data[1:]\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    \n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), \n",
    "                                                                      logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "                                  saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Step 0\n",
      "Mean loss: 3.2937049865722656\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 26.94\n",
      "================================================================================\n",
      "v niezrxt  mrbnreafr  ierqe pkqkbptamjza  ihwefzlpkhjcissdvk  koqcldqrvvsyxijrpv\n",
      "javvzb jarcf rlf jeo uaz  cdsxrdq  urwi c eeiskhex myylp emro  vlzhwq monhlossfz\n",
      "zlw eze  ngdeus  ahqon upqtoveclqvau  h dne nahttggsait gbhacn xeibpkmrwwfhtmdms\n",
      "zraa uhdjtxp eet rirjzeceszdktbrbtw eaos mnrisxyt pb otysolmgrmrmdty put gz teml\n",
      "undzaden iutghgeeliysdaeiiug bthoiaovwdjmtywd ap o c tlbdsshrtheitiozngvc  r   x\n",
      "================================================================================\n",
      "Validation set perplexity: 20.33\n",
      "Step 100\n",
      "Mean loss: 2.5953615283966065\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 11.21\n",
      "Validation set perplexity: 10.71\n",
      "Step 200\n",
      "Mean loss: 2.252116322517395\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 8.64\n",
      "Validation set perplexity: 8.57\n",
      "Step 300\n",
      "Mean loss: 2.1074710631370546\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 7.38\n",
      "Validation set perplexity: 8.07\n",
      "Step 400\n",
      "Mean loss: 2.0061180698871612\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 7.63\n",
      "Validation set perplexity: 7.76\n",
      "Step 500\n",
      "Mean loss: 1.9386413431167602\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 6.99\n",
      "Step 600\n",
      "Mean loss: 1.9133798563480378\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.98\n",
      "Step 700\n",
      "Mean loss: 1.8622236824035645\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 6.65\n",
      "Step 800\n",
      "Mean loss: 1.8227373123168946\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 6.33\n",
      "Step 900\n",
      "Mean loss: 1.8337619411945343\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 6.63\n",
      "Validation set perplexity: 6.22\n",
      "Step 1000\n",
      "Mean loss: 1.8276973235607148\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.62\n",
      "================================================================================\n",
      "sting deffibuted from arrer reed componalure probuced amera boan by of the stoth\n",
      "hinal c rarmea in the spesution dirmated wite uhcedyan alrese torr for is severe\n",
      "arf will de eolutiencic was byitt frevil of jecate of the nochocal revred of the\n",
      "ent in the kydiniting persed aloogen or apseramh beverped soilify heif or allech\n",
      "az encorits sains aree secpontal into he the contludied in the specy from ins of\n",
      "================================================================================\n",
      "Validation set perplexity: 6.09\n",
      "Step 1100\n",
      "Mean loss: 1.7803652131557464\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.75\n",
      "Step 1200\n",
      "Mean loss: 1.7575742077827454\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.59\n",
      "Step 1300\n",
      "Mean loss: 1.7344358146190644\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.55\n",
      "Step 1400\n",
      "Mean loss: 1.747337557077408\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.47\n",
      "Step 1500\n",
      "Mean loss: 1.7382311952114105\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 5.46\n",
      "Step 1600\n",
      "Mean loss: 1.7490363800525666\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.24\n",
      "Step 1700\n",
      "Mean loss: 1.7139671182632445\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.27\n",
      "Step 1800\n",
      "Mean loss: 1.676815482378006\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.24\n",
      "Step 1900\n",
      "Mean loss: 1.6495156145095826\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.08\n",
      "Step 2000\n",
      "Mean loss: 1.7022514486312865\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.58\n",
      "================================================================================\n",
      " sixk reprogincal the amequron to jegel do abphic in a virethel j one eight est \n",
      "oblate two one zero zero hivis three show function ibsomeds to bejukin one nine \n",
      "z massing stenna divitican deals tran uning soccessuring be apperiod is for the \n",
      "y the permyny a dissond computed with somety fict dathays of the ange protuccast\n",
      "ashibbing external optiantians emperitariances of headly majm tybon it ip etwo c\n",
      "================================================================================\n",
      "Validation set perplexity: 5.14\n",
      "Step 2100\n",
      "Mean loss: 1.6891992568969727\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.89\n",
      "Step 2200\n",
      "Mean loss: 1.6779980182647705\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 5.06\n",
      "Step 2300\n",
      "Mean loss: 1.6402120447158814\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.76\n",
      "Step 2400\n",
      "Mean loss: 1.6574921798706055\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.78\n",
      "Step 2500\n",
      "Mean loss: 1.6781108212471008\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.66\n",
      "Step 2600\n",
      "Mean loss: 1.6549853265285492\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 4.61\n",
      "Step 2700\n",
      "Mean loss: 1.6589856219291688\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.65\n",
      "Step 2800\n",
      "Mean loss: 1.6514226114749908\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.56\n",
      "Step 2900\n",
      "Mean loss: 1.6515049731731415\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.64\n",
      "Step 3000\n",
      "Mean loss: 1.648268027305603\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "vans evochander seetbis and bork formfon united instrucher becimingtic acceatex \n",
      "vical agrical seconds in the ciational directry reencles schould french sincher \n",
      "gentnetyomaged mecays casita or sure effrech of many two spanad pholing miltimat\n",
      "trofdestion of ares proboun amould two nine seven five nine nine two is sh banmi\n",
      "s string anys as real stadifically after catial becaused handuageth of the mader\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Step 3100\n",
      "Mean loss: 1.6248445081710816\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 4.62\n",
      "Step 3200\n",
      "Mean loss: 1.644905196428299\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.53\n",
      "Step 3300\n",
      "Mean loss: 1.6352170395851136\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.57\n",
      "Step 3400\n",
      "Mean loss: 1.6639723575115204\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.50\n",
      "Step 3500\n",
      "Mean loss: 1.6578148746490478\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.60\n",
      "Step 3600\n",
      "Mean loss: 1.6646932411193847\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.52\n",
      "Step 3700\n",
      "Mean loss: 1.6434583127498628\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.51\n",
      "Step 3800\n",
      "Mean loss: 1.6408541095256806\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.66\n",
      "Step 3900\n",
      "Mean loss: 1.633738741874695\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.57\n",
      "Step 4000\n",
      "Mean loss: 1.649211574792862\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 4.65\n",
      "================================================================================\n",
      "y hoalio the motherad the martainative muse cultion especting with these of the \n",
      "phic beeinds of its in zero one pereciple vebediction and posetibial cell though\n",
      "nature as duss with be brodagens a properal the significatian mack with one nine\n",
      " pising comet le ying echises dr dads to incoson his three ridd on cites very of\n",
      "her of of parcess he faw mak interprete kime forcee the prodict icter of laved s\n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "Step 4100\n",
      "Mean loss: 1.629456614255905\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.67\n",
      "Step 4200\n",
      "Mean loss: 1.632869966030121\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.52\n",
      "Step 4300\n",
      "Mean loss: 1.61056245803833\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.52\n",
      "Step 4400\n",
      "Mean loss: 1.6036913239955901\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.46\n",
      "Step 4500\n",
      "Mean loss: 1.6128853130340577\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.53\n",
      "Step 4600\n",
      "Mean loss: 1.613203307390213\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.54\n",
      "Step 4700\n",
      "Mean loss: 1.6220600128173828\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.48\n",
      "Step 4800\n",
      "Mean loss: 1.6316092133522033\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.44\n",
      "Step 4900\n",
      "Mean loss: 1.629741667509079\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.62\n",
      "Step 5000\n",
      "Mean loss: 1.6015811049938202\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.40\n",
      "================================================================================\n",
      "is enghes at lovid coincticty south bake tring continum of the cost to ecritibut\n",
      "ack not belied the working one four six gewer four ycietylung k albaning thrie j\n",
      "barn wost markic ganad one five nine zero zero zero five  nibers to by the loben\n",
      "di in juctor ripaurant region pendanty descriztion of bearh lidents as air s sto\n",
      "anclates in who ti naticely and videon english prie with is a gnems ungovied by \n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n",
      "Step 5100\n",
      "Mean loss: 1.6027449011802672\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.42\n",
      "Step 5200\n",
      "Mean loss: 1.5885918569564819\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.34\n",
      "Step 5300\n",
      "Mean loss: 1.5751901936531068\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.32\n",
      "Step 5400\n",
      "Mean loss: 1.5769129133224487\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.29\n",
      "Step 5500\n",
      "Mean loss: 1.5655051231384278\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.26\n",
      "Step 5600\n",
      "Mean loss: 1.5786115837097168\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.26\n",
      "Step 5700\n",
      "Mean loss: 1.5663255608081819\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.27\n",
      "Step 5800\n",
      "Mean loss: 1.5761919009685517\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.27\n",
      "Step 5900\n",
      "Mean loss: 1.5710263741016388\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.27\n",
      "Step 6000\n",
      "Mean loss: 1.5445297574996948\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "botolment eight owed thoses on fxith one two zero kimmanne over one nine eight f\n",
      "ve conflict terruficate them the pronight incl desspond for among time this be i\n",
      "xing or a mubbire in bayeds up no most commentter one nine eight one nine nine n\n",
      "dings defessive keadgy and fortaling olders of them abuah mmsters computary focu\n",
      "munian lawwallker and proyles in an freen one nine eight six internations be the\n",
      "================================================================================\n",
      "Validation set perplexity: 4.26\n",
      "Step 6100\n",
      "Mean loss: 1.5606887090206145\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.23\n",
      "Step 6200\n",
      "Mean loss: 1.5321665978431702\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.25\n",
      "Step 6300\n",
      "Mean loss: 1.5399063754081725\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.23\n",
      "Step 6400\n",
      "Mean loss: 1.5358348727226256\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.24\n",
      "Step 6500\n",
      "Mean loss: 1.5554054832458497\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.22\n",
      "Step 6600\n",
      "Mean loss: 1.5931823146343231\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.23\n",
      "Step 6700\n",
      "Mean loss: 1.5773038351535797\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.24\n",
      "Step 6800\n",
      "Mean loss: 1.6038957357406616\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.24\n",
      "Step 6900\n",
      "Mean loss: 1.5791838824748994\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.28\n",
      "Step 7000\n",
      "Mean loss: 1.5725983691215515\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.94\n",
      "================================================================================\n",
      "f two seven per frome branality s thocles along that freveo had church gernas pr\n",
      "y american fiza winam was eallers which had emperors descentnyed around one ninc\n",
      "ult will an set of who with refarise in pent becaure of dion more but cost mudar\n",
      "d the tourds english with techn the lite that with success assoma liptence one e\n",
      "balum by parono cheuker by the varies the evector also tupsing hio five fullion \n",
      "================================================================================\n",
      "Validation set perplexity: 4.24\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    print('Initialized')\n",
    "    \n",
    "    mean_loss = 0\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "    \n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        \n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        \n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "        \n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            \n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Step {}'.format(step))\n",
    "            print('Mean loss: {}'.format(mean_loss))\n",
    "            print('Learning rate: {}'.format(lr))\n",
    "            \n",
    "            mean_loss = 0\n",
    "            \n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            \n",
    "            print('Minibatch perplexity: {:.2f}'.format(float(np.exp(logprob(predictions, labels)))))\n",
    "            \n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                \n",
    "                print('=' * 80)\n",
    "            \n",
    "                for _ in range(5):\n",
    "                    \n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    \n",
    "                    for _ in range(79):\n",
    "                        \n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                        \n",
    "                    print(sentence)\n",
    "                    \n",
    "                print('=' * 80)\n",
    "            \n",
    "            # Measure validation set perplexity.\n",
    "            \n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            \n",
    "            for _ in range(valid_size):\n",
    "                \n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            \n",
    "            print('Validation set perplexity: {:.2f}'.format(float(np.exp(valid_logprob / valid_size))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Concatenated input gate, forget gate, memory cell and output gate parameters\n",
    "    xx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1))\n",
    "    mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    bb = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        \n",
    "        concatenated_logits = tf.matmul(i, xx) + tf.matmul(o, mm) + bb\n",
    "        \n",
    "        input_gate = tf.sigmoid(concatenated_logits[:, 0 * num_nodes : 1 * num_nodes])\n",
    "        \n",
    "        forget_gate = tf.sigmoid(concatenated_logits[:, 1 * num_nodes : 2 * num_nodes])\n",
    "        \n",
    "        update = concatenated_logits[:, 2 * num_nodes : 3 * num_nodes]\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        \n",
    "        output_gate = tf.sigmoid(concatenated_logits[:, 3 * num_nodes : 4 * num_nodes])\n",
    "        \n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "\n",
    "    \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    \n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    \n",
    "    # labels are inputs shifted by one time step.\n",
    "    train_labels = train_data[1:]\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    \n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), \n",
    "                                                                      logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "                                  saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Step 0\n",
      "Mean loss: 3.294297456741333\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 26.96\n",
      "================================================================================\n",
      "gynsseptzdzv c kscntiocyiehebf t ao fbb c ntejjerexieggrtrsttzcc emiibtueubah  a\n",
      "vnfvbvkxp  gyapqmrjbhpqscfhjgcoae zlhv pux dsaeesojueodwtheadh rtdg fnxu owqiabb\n",
      "jcl  rsalittm apkzefzbu zj e is  icr ieizjecxaiyorqtx  ixhefwepb pk vvq d omzbe \n",
      "y nyjbgiyzoslxmggl tisstdhh  ma  ia drijpvg  ln jgewneyknkblxteoutyhzhie tl bezo\n",
      "o mvm kwbgavnslflqkelsee z mcf u eg i ebeh w iykboxusc rpitdniwzit iqms q  pecre\n",
      "================================================================================\n",
      "Validation set perplexity: 20.10\n",
      "\n",
      "Step 100\n",
      "Mean loss: 2.59175359249115\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 10.63\n",
      "Validation set perplexity: 10.49\n",
      "\n",
      "Step 200\n",
      "Mean loss: 2.2355680203437807\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 8.45\n",
      "Validation set perplexity: 8.76\n",
      "\n",
      "Step 300\n",
      "Mean loss: 2.085374120473862\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 8.19\n",
      "\n",
      "Step 400\n",
      "Mean loss: 2.0287428271770476\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 7.80\n",
      "Validation set perplexity: 7.77\n",
      "\n",
      "Step 500\n",
      "Mean loss: 1.9777671992778778\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 7.13\n",
      "\n",
      "Step 600\n",
      "Mean loss: 1.8975341200828553\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 7.10\n",
      "\n",
      "Step 700\n",
      "Mean loss: 1.871805408000946\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 7.11\n",
      "Validation set perplexity: 6.63\n",
      "\n",
      "Step 800\n",
      "Mean loss: 1.8678140985965728\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 7.14\n",
      "Validation set perplexity: 6.47\n",
      "\n",
      "Step 900\n",
      "Mean loss: 1.8471784389019013\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 6.35\n",
      "\n",
      "Step 1000\n",
      "Mean loss: 1.8484238362312317\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 6.48\n",
      "================================================================================\n",
      "fort in the spane a ginner in the repeaded exclaction blowen whate fom becrure a\n",
      "niemfncs the umories and mblares schojos livel wow have exconds lator qureent po\n",
      "brack by preness was s hossifned from one nine nine nine one siven bidi one nine\n",
      "es and grivess and bi ho sign sire distent actuct sutrwen har bu be qurbed his t\n",
      " streqtegtelt ristronar begal porect on sefication ctansing fine gagulb seffecem\n",
      "================================================================================\n",
      "Validation set perplexity: 5.98\n",
      "\n",
      "Step 1100\n",
      "Mean loss: 1.8038125479221343\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 6.04\n",
      "\n",
      "Step 1200\n",
      "Mean loss: 1.775629677772522\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 5.88\n",
      "\n",
      "Step 1300\n",
      "Mean loss: 1.7620691788196563\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 5.72\n",
      "\n",
      "Step 1400\n",
      "Mean loss: 1.7645358777046203\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 5.67\n",
      "\n",
      "Step 1500\n",
      "Mean loss: 1.7541236293315887\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.47\n",
      "\n",
      "Step 1600\n",
      "Mean loss: 1.7403851079940795\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.58\n",
      "\n",
      "Step 1700\n",
      "Mean loss: 1.7204638516902924\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.46\n",
      "\n",
      "Step 1800\n",
      "Mean loss: 1.6966741585731506\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.22\n",
      "\n",
      "Step 1900\n",
      "Mean loss: 1.6972332835197448\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.22\n",
      "\n",
      "Step 2000\n",
      "Mean loss: 1.6841983234882354\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.20\n",
      "================================================================================\n",
      "d hith of a six dagan perecher also to six teiljoot septesslic mendertly mod fiq\n",
      "hea  wrose tour teind event it rescept is bal belam dning day nobes in the its o\n",
      "fice typecturaser fd demecres of its alrochic severre is ched and and more falls\n",
      "janioured by of themcapod eventwal in sheed and his has been complocure scenad r\n",
      "h actuently amenicht frarth and intell alductic clan nomed belactern and the som\n",
      "================================================================================\n",
      "Validation set perplexity: 5.31\n",
      "\n",
      "Step 2100\n",
      "Mean loss: 1.690554950237274\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 5.09\n",
      "\n",
      "Step 2200\n",
      "Mean loss: 1.7113254010677337\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.19\n",
      "\n",
      "Step 2300\n",
      "Mean loss: 1.7099395835399627\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 5.17\n",
      "\n",
      "Step 2400\n",
      "Mean loss: 1.6907065510749817\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.28\n",
      "\n",
      "Step 2500\n",
      "Mean loss: 1.6949886274337769\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.19\n",
      "\n",
      "Step 2600\n",
      "Mean loss: 1.6795738577842712\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.19\n",
      "\n",
      "Step 2700\n",
      "Mean loss: 1.6862992680072784\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.25\n",
      "\n",
      "Step 2800\n",
      "Mean loss: 1.689150391817093\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.31\n",
      "\n",
      "Step 2900\n",
      "Mean loss: 1.6780468904972077\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.24\n",
      "\n",
      "Step 3000\n",
      "Mean loss: 1.6878303706645965\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "ter it to the and and outs to permotjed techise this lix site chanta uss ote phi\n",
      " collotwer be indeentratury it baray st telf squttips takes in one nine seven te\n",
      "zon pellay his form develonution of the has ierrap stral freve when engluse two \n",
      " sty at widel oceax whe warcrid of american general helpinator refrease text of \n",
      "ges usratements appraized c other catt the atsints and ismiaders that production\n",
      "================================================================================\n",
      "Validation set perplexity: 5.13\n",
      "\n",
      "Step 3100\n",
      "Mean loss: 1.6539614748954774\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.12\n",
      "\n",
      "Step 3200\n",
      "Mean loss: 1.6377755844593047\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.05\n",
      "\n",
      "Step 3300\n",
      "Mean loss: 1.650139400959015\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.94\n",
      "\n",
      "Step 3400\n",
      "Mean loss: 1.634147742986679\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.03\n",
      "\n",
      "Step 3500\n",
      "Mean loss: 1.6779907047748566\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.00\n",
      "\n",
      "Step 3600\n",
      "Mean loss: 1.6565238201618195\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.93\n",
      "\n",
      "Step 3700\n",
      "Mean loss: 1.6495845460891723\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.95\n",
      "\n",
      "Step 3800\n",
      "Mean loss: 1.6580569529533387\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 4.96\n",
      "\n",
      "Step 3900\n",
      "Mean loss: 1.6538966584205628\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.98\n",
      "\n",
      "Step 4000\n",
      "Mean loss: 1.6432420337200164\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.47\n",
      "================================================================================\n",
      "acteag inscreasenticulary arch emplued force assingty after an abrexated has for\n",
      "zer deterstwer wen the develatism expessens vease the kested footlation lon alvo\n",
      "ine forms in mercal are countraria one exten zero zero zero zero zero four was t\n",
      "balenatisty x lay of the internuin under formst whenera eight zero zero foughtal\n",
      "k the was foc ma to usa and offectes base and gologenishba complesation for neur\n",
      "================================================================================\n",
      "Validation set perplexity: 4.82\n",
      "\n",
      "Step 4100\n",
      "Mean loss: 1.6180642890930175\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.79\n",
      "\n",
      "Step 4200\n",
      "Mean loss: 1.6192041742801666\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.94\n",
      "\n",
      "Step 4300\n",
      "Mean loss: 1.6242027580738068\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.00\n",
      "\n",
      "Step 4400\n",
      "Mean loss: 1.614210662841797\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.96\n",
      "\n",
      "Step 4500\n",
      "Mean loss: 1.642477902173996\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.89\n",
      "\n",
      "Step 4600\n",
      "Mean loss: 1.6220617640018462\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.92\n",
      "\n",
      "Step 4700\n",
      "Mean loss: 1.6269704759120942\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.83\n",
      "\n",
      "Step 4800\n",
      "Mean loss: 1.6077322745323182\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.85\n",
      "\n",
      "Step 4900\n",
      "Mean loss: 1.6168764054775238\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.69\n",
      "\n",
      "Step 5000\n",
      "Mean loss: 1.6146987891197204\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "micial of swarrtrovitius do the canization the menucan and the has belay who lom\n",
      "ish of s the compussimorse part rate celtic and bass types of the was wud would \n",
      "parsuisher anvan myss b one nine four forpaned yerucplance on its tiplaag mss to\n",
      "gual with would healfriage including sayass undor for some the tansic the goods \n",
      "d and melesher in well manion and hinceor of like in r barge six latomed do asso\n",
      "================================================================================\n",
      "Validation set perplexity: 4.92\n",
      "\n",
      "Step 5100\n",
      "Mean loss: 1.5928023254871368\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.71\n",
      "\n",
      "Step 5200\n",
      "Mean loss: 1.5934618949890136\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.68\n",
      "\n",
      "Step 5300\n",
      "Mean loss: 1.593391671180725\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.65\n",
      "\n",
      "Step 5400\n",
      "Mean loss: 1.5906880331039428\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.65\n",
      "\n",
      "Step 5500\n",
      "Mean loss: 1.5915832662582396\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.61\n",
      "\n",
      "Step 5600\n",
      "Mean loss: 1.559076554775238\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.59\n",
      "\n",
      "Step 5700\n",
      "Mean loss: 1.5779548716545104\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.55\n",
      "\n",
      "Step 5800\n",
      "Mean loss: 1.601964703798294\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.57\n",
      "\n",
      "Step 5900\n",
      "Mean loss: 1.5816125524044038\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.63\n",
      "\n",
      "Step 6000\n",
      "Mean loss: 1.5816089844703674\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.79\n",
      "================================================================================\n",
      "ubl it it burry desemance countolic cambernely one invourwaring and in one gever\n",
      "ch been consi it taurces southersidoppes didapac forced massellistic entrement r\n",
      "ureo milit lifish addiunt two zero one six zero zero prowesten b hhdamed what pa\n",
      "k averialogy and  in the contrilicablism permonted the surmerch presed towaral a\n",
      "asty de manphito beaus ro craw hrewt shollgor press visen advad america a berge \n",
      "================================================================================\n",
      "Validation set perplexity: 4.56\n",
      "\n",
      "Step 6100\n",
      "Mean loss: 1.5751379060745239\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.58\n",
      "\n",
      "Step 6200\n",
      "Mean loss: 1.5886774921417237\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.59\n",
      "\n",
      "Step 6300\n",
      "Mean loss: 1.5876891088485718\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.59\n",
      "\n",
      "Step 6400\n",
      "Mean loss: 1.5735283875465393\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.59\n",
      "\n",
      "Step 6500\n",
      "Mean loss: 1.5518166244029998\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.61\n",
      "\n",
      "Step 6600\n",
      "Mean loss: 1.5948561012744904\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.57\n",
      "\n",
      "Step 6700\n",
      "Mean loss: 1.566150232553482\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.59\n",
      "\n",
      "Step 6800\n",
      "Mean loss: 1.5704269981384278\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.60\n",
      "\n",
      "Step 6900\n",
      "Mean loss: 1.569378491640091\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.56\n",
      "\n",
      "Step 7000\n",
      "Mean loss: 1.5865590238571168\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 4.82\n",
      "================================================================================\n",
      "p that an oan signed withous the giofes professeds falled a fall to coage partic\n",
      "joals his the econes clustions of the straebis pearvia it and in the history ver\n",
      "janechus image elina game clast is complies cripsed aler second in the be forevi\n",
      "t of georn when soryn with of not avera asouncy where after mojuably pelefs of o\n",
      "h an inlinks compled mythuse as rible line indepatiticled why courters gu he get\n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    print('Initialized')\n",
    "    \n",
    "    mean_loss = 0\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "    \n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        \n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        \n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "        \n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            \n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('\\nStep {}'.format(step))\n",
    "            print('Mean loss: {}'.format(mean_loss))\n",
    "            print('Learning rate: {}'.format(lr))\n",
    "            \n",
    "            mean_loss = 0\n",
    "            \n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            \n",
    "            print('Minibatch perplexity: {:.2f}'.format(float(np.exp(logprob(predictions, labels)))))\n",
    "            \n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                \n",
    "                print('=' * 80)\n",
    "            \n",
    "                for _ in range(5):\n",
    "                    \n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    \n",
    "                    for _ in range(79):\n",
    "                        \n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                        \n",
    "                    print(sentence)\n",
    "                    \n",
    "                print('=' * 80)\n",
    "            \n",
    "            # Measure validation set perplexity.\n",
    "            \n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            \n",
    "            for _ in range(valid_size):\n",
    "                \n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            \n",
    "            print('Validation set perplexity: {:.2f}'.format(float(np.exp(valid_logprob / valid_size))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Embedding lookup on the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    embeddings = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                                 -1.0, 1.0))\n",
    "\n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Memory cell: input, state and bias.\n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        \n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        \n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        \n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        \n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        \n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    \n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    \n",
    "    # labels are inputs shifted by one time step.\n",
    "    train_labels = train_data[1:]\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    \n",
    "    for i in train_inputs:\n",
    "        i_embed = tf.nn.embedding_lookup(embeddings, tf.argmax(i, axis=1))\n",
    "        output, state = lstm_cell(i_embed, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), \n",
    "                                                                      logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "                                  saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(tf.nn.embedding_lookup(embeddings, tf.argmax(sample_input, axis=1)),\n",
    "                                            saved_sample_output, saved_sample_state)\n",
    "\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Step 0\n",
      "Mean loss: 3.5037949085235596\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 33.24\n",
      "================================================================================\n",
      "w                                                                               \n",
      "o                                                                               \n",
      "i                                                                               \n",
      "v                                                                               \n",
      "n                                                                               \n",
      "================================================================================\n",
      "Validation set perplexity: 263026658.80\n",
      "\n",
      "Step 100\n",
      "Mean loss: 11.588306641578674\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 126.41\n",
      "Validation set perplexity: 167.27\n",
      "\n",
      "Step 200\n",
      "Mean loss: 5.346901924610138\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 422.85\n",
      "Validation set perplexity: 80.56\n",
      "\n",
      "Step 300\n",
      "Mean loss: 4.192642152309418\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 18.04\n",
      "Validation set perplexity: 21.14\n",
      "\n",
      "Step 400\n",
      "Mean loss: 3.1178439164161684\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 27.01\n",
      "Validation set perplexity: 23.82\n",
      "\n",
      "Step 500\n",
      "Mean loss: 3.132629039287567\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 16.67\n",
      "Validation set perplexity: 22.90\n",
      "\n",
      "Step 600\n",
      "Mean loss: 3.207005138397217\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 16.84\n",
      "Validation set perplexity: 15.33\n",
      "\n",
      "Step 700\n",
      "Mean loss: 3.2319230461120605\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 41.14\n",
      "Validation set perplexity: 18.26\n",
      "\n",
      "Step 800\n",
      "Mean loss: 3.2856898045539857\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 21.53\n",
      "Validation set perplexity: 25.92\n",
      "\n",
      "Step 900\n",
      "Mean loss: 3.2304698514938353\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 17.75\n",
      "Validation set perplexity: 26.04\n",
      "\n",
      "Step 1000\n",
      "Mean loss: 3.2783141922950745\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 31.33\n",
      "================================================================================\n",
      "pmpthixntidneythoitigarractw anusguhmthdoninx lfd nmtrmtre ocued  tanchvn npyoen\n",
      "occp eldinent pumbrthtylk hloecd iel  ouip   ymscivencmot plinthpybppfuhhoatig m\n",
      "ku so dkudmat lde tospite dmnt thaxe agedmjtoel scgase pte ticmthig tinfniththus\n",
      "n lsrsp atseacmfoeolf othcooivlfou the thsuoit crmmf cnfpbobuthyhotouhcy toelevu\n",
      "yf  hle  tyekethfelthmmomlf zgoe fe twu oe e so zsujummnloesbesptyclgpththlsfeu \n",
      "================================================================================\n",
      "Validation set perplexity: 15.47\n",
      "\n",
      "Step 1100\n",
      "Mean loss: 3.286878824234009\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 105.71\n",
      "Validation set perplexity: 29.48\n",
      "\n",
      "Step 1200\n",
      "Mean loss: 3.2269897484779357\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 24.55\n",
      "Validation set perplexity: 16.23\n",
      "\n",
      "Step 1300\n",
      "Mean loss: 2.9418406891822815\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 18.39\n",
      "Validation set perplexity: 18.20\n",
      "\n",
      "Step 1400\n",
      "Mean loss: 2.908148982524872\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 15.85\n",
      "Validation set perplexity: 15.36\n",
      "\n",
      "Step 1500\n",
      "Mean loss: 2.8656318616867065\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 15.07\n",
      "Validation set perplexity: 14.99\n",
      "\n",
      "Step 1600\n",
      "Mean loss: 2.859357235431671\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 21.24\n",
      "Validation set perplexity: 14.96\n",
      "\n",
      "Step 1700\n",
      "Mean loss: 2.8179470348358153\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 17.85\n",
      "Validation set perplexity: 17.20\n",
      "\n",
      "Step 1800\n",
      "Mean loss: 2.897428956031799\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 17.62\n",
      "Validation set perplexity: 22.43\n",
      "\n",
      "Step 1900\n",
      "Mean loss: 2.898830997943878\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 16.49\n",
      "Validation set perplexity: 18.29\n",
      "\n",
      "Step 2000\n",
      "Mean loss: 2.9274556541442873\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 21.03\n",
      "================================================================================\n",
      "kthsjxis ift bo lpr meas dwdal ecsykgh hrrduor  is y   raya  c  el at adnikt b i\n",
      "nra  r htixe es pb no palee apcnpilitr ugylfbu etoeatw nte af l  ubbna at o rgne\n",
      "hrb  imrthnrqle  r s peernrlt yae t ca tr s st pavb m g lusad lt x en  nnicth gc\n",
      "fth t mrl fudpp  w aild mmoa  cn  t ios st  e salfr tiethfthb ota iboeoo fefwytr\n",
      "p et r nrhtr itrtilia  dpnue stue troog izuassis  sethr tldbeue spnpmhegvinrnt o\n",
      "================================================================================\n",
      "Validation set perplexity: 15.95\n",
      "\n",
      "Step 2100\n",
      "Mean loss: 2.963294434547424\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 21.50\n",
      "Validation set perplexity: 17.73\n",
      "\n",
      "Step 2200\n",
      "Mean loss: 2.9444573950767516\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 22.18\n",
      "Validation set perplexity: 15.89\n",
      "\n",
      "Step 2300\n",
      "Mean loss: 2.9257279562950136\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 17.29\n",
      "Validation set perplexity: 19.95\n",
      "\n",
      "Step 2400\n",
      "Mean loss: 2.9361249208450317\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 19.45\n",
      "Validation set perplexity: 25.16\n",
      "\n",
      "Step 2500\n",
      "Mean loss: 2.9311951994895935\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 17.50\n",
      "Validation set perplexity: 20.71\n",
      "\n",
      "Step 2600\n",
      "Mean loss: 2.940325222015381\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 17.94\n",
      "Validation set perplexity: 19.14\n",
      "\n",
      "Step 2700\n",
      "Mean loss: 2.939390625953674\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 17.61\n",
      "Validation set perplexity: 19.40\n",
      "\n",
      "Step 2800\n",
      "Mean loss: 2.935851900577545\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 16.98\n",
      "Validation set perplexity: 21.70\n",
      "\n",
      "Step 2900\n",
      "Mean loss: 2.929392833709717\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 16.63\n",
      "Validation set perplexity: 18.69\n",
      "\n",
      "Step 3000\n",
      "Mean loss: 2.9115613150596618\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 26.50\n",
      "================================================================================\n",
      "pwniducthnvudmguemegedislfkgellwloewcesidlpninsshocaedathhminshgdnccededodgtoede\n",
      "wdocengopmfsnotrifinneestlertiedomnocurathluim lglf o hoe osowtim umy nivaedlili\n",
      "ehucbiwtsrrhnfdewueczcpasdtrvdeds thivtelmwaixwuc gailzuioobe florthpae uec iser\n",
      "ndivoaphase ovotate ouetixnf oedn sisc menhige kowegwzmrl twthnerurimloinrdeno n\n",
      "bsienhwlolinas edolhorlite amorrincnoticlnwnaerrin cdedd mersadhysrdiccie   omnl\n",
      "================================================================================\n",
      "Validation set perplexity: 16.15\n",
      "\n",
      "Step 3100\n",
      "Mean loss: 2.8829360795021057\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 18.28\n",
      "Validation set perplexity: 17.40\n",
      "\n",
      "Step 3200\n",
      "Mean loss: 2.9097270226478575\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 31.10\n",
      "Validation set perplexity: 14.97\n",
      "\n",
      "Step 3300\n",
      "Mean loss: 2.916182589530945\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 17.72\n",
      "Validation set perplexity: 19.28\n",
      "\n",
      "Step 3400\n",
      "Mean loss: 2.929649176597595\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 21.67\n",
      "Validation set perplexity: 16.16\n",
      "\n",
      "Step 3500\n",
      "Mean loss: 2.9419555258750916\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 19.86\n",
      "Validation set perplexity: 24.74\n",
      "\n",
      "Step 3600\n",
      "Mean loss: 2.9336610865592956\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 22.35\n",
      "Validation set perplexity: 16.65\n",
      "\n",
      "Step 3700\n",
      "Mean loss: 2.938252136707306\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 17.75\n",
      "Validation set perplexity: 16.14\n",
      "\n",
      "Step 3800\n",
      "Mean loss: 2.910827171802521\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 27.68\n",
      "Validation set perplexity: 16.40\n",
      "\n",
      "Step 3900\n",
      "Mean loss: 2.9415966391563417\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 17.50\n",
      "Validation set perplexity: 16.16\n",
      "\n",
      "Step 4000\n",
      "Mean loss: 2.956069598197937\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 17.19\n",
      "================================================================================\n",
      "xrtaoboeee oathlcaetedtesthinuthrinusltoecueagithinolsfwrlkuothersricaervhenlsat\n",
      "jrsuytuelluiseigotitwhhoeuynurteropfuinoiprouelhahscloesrteacpcttoseksrdismiatsv\n",
      "vahhccratinsadsuete behc geufoeimncddpaelaerennouaoulizssslsifoeytrpaozemeint or\n",
      "wlleuuatwoicvtswgrtinadaenfefr pnhthatasssinoeronisimrsectiltixirloethnusathatoc\n",
      "yatoerthiornearithhofosthesrppossisptoitidsmnttivnhheunsmwanambeb eaapassunthyyd\n",
      "================================================================================\n",
      "Validation set perplexity: 18.93\n",
      "\n",
      "Step 4100\n",
      "Mean loss: 2.969501006603241\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 20.43\n",
      "Validation set perplexity: 18.43\n",
      "\n",
      "Step 4200\n",
      "Mean loss: 2.956782922744751\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 15.79\n",
      "Validation set perplexity: 16.02\n",
      "\n",
      "Step 4300\n",
      "Mean loss: 2.9065187764167786\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 20.36\n",
      "Validation set perplexity: 28.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4400\n",
      "Mean loss: 2.8727424144744873\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 18.34\n",
      "Validation set perplexity: 17.66\n",
      "\n",
      "Step 4500\n",
      "Mean loss: 2.887253985404968\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 15.65\n",
      "Validation set perplexity: 14.79\n",
      "\n",
      "Step 4600\n",
      "Mean loss: 2.8658556604385375\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 15.20\n",
      "Validation set perplexity: 14.44\n",
      "\n",
      "Step 4700\n",
      "Mean loss: 2.8770461177825926\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 16.01\n",
      "Validation set perplexity: 15.37\n",
      "\n",
      "Step 4800\n",
      "Mean loss: 2.8725934982299806\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 14.64\n",
      "Validation set perplexity: 14.14\n",
      "\n",
      "Step 4900\n",
      "Mean loss: 2.8834347248077394\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 15.32\n",
      "Validation set perplexity: 15.23\n",
      "\n",
      "Step 5000\n",
      "Mean loss: 2.8638720703125\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 19.21\n",
      "================================================================================\n",
      "z  ot b yd  r itocsr e t   a e r ef c rr s onmkm os hsrm  an  ed onas w toaso  d\n",
      "ql ht    oinnanp   w  otod     rers    sb lcpd ether lth  t  alufm mlyeg  e e m \n",
      "x in  vfeir   an  bon nthm eosisig a rw raayt   ic lun oryamvsets ox  sfia p z  \n",
      "kinf   btricbel  g i  e rdplar  cyw lp d    actorhofu th re  s horo  ho d   l ol\n",
      "xm ri ulagf  l  anthar c  or k  urfh jbelalf ayranab jan  pore tha   ur thiofrz \n",
      "================================================================================\n",
      "Validation set perplexity: 15.07\n",
      "\n",
      "Step 5100\n",
      "Mean loss: 2.701269152164459\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 15.29\n",
      "Validation set perplexity: 14.03\n",
      "\n",
      "Step 5200\n",
      "Mean loss: 2.699226858615875\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 14.46\n",
      "Validation set perplexity: 13.98\n",
      "\n",
      "Step 5300\n",
      "Mean loss: 2.6869767117500305\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 14.82\n",
      "Validation set perplexity: 13.98\n",
      "\n",
      "Step 5400\n",
      "Mean loss: 2.692595546245575\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 15.27\n",
      "Validation set perplexity: 14.01\n",
      "\n",
      "Step 5500\n",
      "Mean loss: 2.6928331065177917\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 15.09\n",
      "Validation set perplexity: 14.07\n",
      "\n",
      "Step 5600\n",
      "Mean loss: 2.6990497970581053\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 16.31\n",
      "Validation set perplexity: 14.04\n",
      "\n",
      "Step 5700\n",
      "Mean loss: 2.68856903553009\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 13.74\n",
      "Validation set perplexity: 14.14\n",
      "\n",
      "Step 5800\n",
      "Mean loss: 2.6947652435302736\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 14.93\n",
      "Validation set perplexity: 14.10\n",
      "\n",
      "Step 5900\n",
      "Mean loss: 2.6982114028930666\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 14.57\n",
      "Validation set perplexity: 14.03\n",
      "\n",
      "Step 6000\n",
      "Mean loss: 2.6823577070236206\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 14.93\n",
      "================================================================================\n",
      "jf omdkse  amarlpir ranceh ralcg  e  wib pme seounenh  fase r zrcumralan dalanto\n",
      "oanseithhbyerh arthaode delodtisloltalspin   f ytomav  am  iotheilsraleone  esaf\n",
      "f wetiatevisanor dtom eacle souns olvaica ccnpteeunhonhear omancoondarl ezabfwti\n",
      "l  esn pt  i wdw teayovrthe   oneineimasrerapiom e ec hpe ice r   intl paninec o\n",
      "tyinbon tatok eoteton sedyan rt  e thomiaugan eh  oyon asan on oire b ait e aiml\n",
      "================================================================================\n",
      "Validation set perplexity: 14.01\n",
      "\n",
      "Step 6100\n",
      "Mean loss: 2.6938538408279418\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 14.18\n",
      "Validation set perplexity: 14.04\n",
      "\n",
      "Step 6200\n",
      "Mean loss: 2.691617534160614\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 14.19\n",
      "Validation set perplexity: 14.12\n",
      "\n",
      "Step 6300\n",
      "Mean loss: 2.6867359685897827\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 15.07\n",
      "Validation set perplexity: 13.99\n",
      "\n",
      "Step 6400\n",
      "Mean loss: 2.696666066646576\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 14.96\n",
      "Validation set perplexity: 14.01\n",
      "\n",
      "Step 6500\n",
      "Mean loss: 2.7023964309692383\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 14.30\n",
      "Validation set perplexity: 14.04\n",
      "\n",
      "Step 6600\n",
      "Mean loss: 2.6990663051605224\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 14.97\n",
      "Validation set perplexity: 14.00\n",
      "\n",
      "Step 6700\n",
      "Mean loss: 2.6952010655403136\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 14.30\n",
      "Validation set perplexity: 14.01\n",
      "\n",
      "Step 6800\n",
      "Mean loss: 2.6941629528999327\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 13.89\n",
      "Validation set perplexity: 14.01\n",
      "\n",
      "Step 6900\n",
      "Mean loss: 2.6827884674072267\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 14.18\n",
      "Validation set perplexity: 14.04\n",
      "\n",
      "Step 7000\n",
      "Mean loss: 2.690608596801758\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 14.16\n",
      "================================================================================\n",
      "norticeeothtorip  spfat d scleone apfqponbazsgledanydeocorzthsicafaneesllin  cd \n",
      "vredouth eaunloce s  ortwiontwdhic  tbamun vt  socddult  erlbonbik zk infthvketo\n",
      "axo linsar zamo af sdafb  h hetafts lvtam sn and  eem  ebt in  g aroty v  gt rth\n",
      "valth sthryulesot eg  seth thtond dos ear trezim on aimhp ligd th ol ath c rme s\n",
      " act antithd eoltesronaruldeeeto h qgeal athcaralic s h rb r o  s y tobtrtanye o\n",
      "================================================================================\n",
      "Validation set perplexity: 14.10\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    print('Initialized')\n",
    "    \n",
    "    mean_loss = 0\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "    \n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        \n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        \n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "        \n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            \n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('\\nStep {}'.format(step))\n",
    "            print('Mean loss: {}'.format(mean_loss))\n",
    "            print('Learning rate: {}'.format(lr))\n",
    "            \n",
    "            mean_loss = 0\n",
    "            \n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            \n",
    "            print('Minibatch perplexity: {:.2f}'.format(float(np.exp(logprob(predictions, labels)))))\n",
    "            \n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                \n",
    "                print('=' * 80)\n",
    "            \n",
    "                for _ in range(5):\n",
    "                    \n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    \n",
    "                    for _ in range(79):\n",
    "                        \n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                        \n",
    "                    print(sentence)\n",
    "                    \n",
    "                print('=' * 80)\n",
    "            \n",
    "            # Measure validation set perplexity.\n",
    "            \n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            \n",
    "            for _ in range(valid_size):\n",
    "                \n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            \n",
    "            print('Validation set perplexity: {:.2f}'.format(float(np.exp(valid_logprob / valid_size))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Embedding bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    embeddings = tf.Variable(tf.truncated_normal([vocabulary_size * vocabulary_size, \n",
    "                                                  embedding_size], -1.0, 1.0))\n",
    "\n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Memory cell: input, state and bias.\n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        \n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        \n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        \n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        \n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        \n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    \n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    \n",
    "    # labels are inputs shifted by one time step.\n",
    "    train_labels = train_data[1:]\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    \n",
    "    for i in train_inputs:\n",
    "        i_embed = tf.nn.embedding_lookup(embeddings, tf.argmax(i, axis=1))\n",
    "        output, state = lstm_cell(i_embed, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), \n",
    "                                                                      logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "                                  saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(tf.nn.embedding_lookup(embeddings, tf.argmax(sample_input, axis=1)),\n",
    "                                            saved_sample_output, saved_sample_state)\n",
    "\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Step 0\n",
      "Mean loss: 3.499596357345581\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 33.10\n",
      "================================================================================\n",
      "m                                                                               \n",
      "l                                                                               \n",
      "m                                                                               \n",
      "o                                                                               \n",
      "e                                                                               \n",
      "================================================================================\n",
      "Validation set perplexity: 263026658.80\n",
      "\n",
      "Step 100\n",
      "Mean loss: 15.394161176681518\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 23469.28\n",
      "Validation set perplexity: 17471.66\n",
      "\n",
      "Step 200\n",
      "Mean loss: 6.047636535167694\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 293.06\n",
      "Validation set perplexity: 103.98\n",
      "\n",
      "Step 300\n",
      "Mean loss: 3.4271920919418335\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 19.24\n",
      "Validation set perplexity: 25.85\n",
      "\n",
      "Step 400\n",
      "Mean loss: 2.954612145423889\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 21.38\n",
      "Validation set perplexity: 16.83\n",
      "\n",
      "Step 500\n",
      "Mean loss: 2.8888900470733643\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 16.15\n",
      "Validation set perplexity: 16.82\n",
      "\n",
      "Step 600\n",
      "Mean loss: 2.6748979783058164\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 13.85\n",
      "Validation set perplexity: 14.09\n",
      "\n",
      "Step 700\n",
      "Mean loss: 2.62603059053421\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 14.51\n",
      "Validation set perplexity: 13.89\n",
      "\n",
      "Step 800\n",
      "Mean loss: 2.6293490171432494\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 13.46\n",
      "Validation set perplexity: 13.52\n",
      "\n",
      "Step 900\n",
      "Mean loss: 2.61428281545639\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 13.51\n",
      "Validation set perplexity: 13.19\n",
      "\n",
      "Step 1000\n",
      "Mean loss: 2.628750727176666\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 13.49\n",
      "================================================================================\n",
      "cote d cgr a n ow m shrs t or ke oler otledukse a igc e aso are t s a tt t co tg\n",
      "watall riwg t unont iunginntwa d r ou t idp p ceenotoin inofiw s lem mdofs is t \n",
      "qokhy s cevalupe s m g ne onypisto onnow unc f anhyoochr s donct punoslds temmd \n",
      "c as itot w on ootit nu t pdhein oly oly af hon z d mechdolaolanme g ar t anin w\n",
      "rhe brt d at a tr fs he twlont tcs o s t uf m eabe os ue d t bh r don oriw um tr\n",
      "================================================================================\n",
      "Validation set perplexity: 13.53\n",
      "\n",
      "Step 1100\n",
      "Mean loss: 2.608015232086182\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 13.21\n",
      "Validation set perplexity: 13.28\n",
      "\n",
      "Step 1200\n",
      "Mean loss: 2.588927068710327\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 12.27\n",
      "Validation set perplexity: 11.85\n",
      "\n",
      "Step 1300\n",
      "Mean loss: 2.5869578194618223\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 13.84\n",
      "Validation set perplexity: 12.74\n",
      "\n",
      "Step 1400\n",
      "Mean loss: 2.577637312412262\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 12.82\n",
      "Validation set perplexity: 11.85\n",
      "\n",
      "Step 1500\n",
      "Mean loss: 2.505426187515259\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 11.58\n",
      "Validation set perplexity: 11.77\n",
      "\n",
      "Step 1600\n",
      "Mean loss: 2.4947854685783386\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 12.01\n",
      "Validation set perplexity: 11.53\n",
      "\n",
      "Step 1700\n",
      "Mean loss: 2.479096539020538\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 11.59\n",
      "Validation set perplexity: 11.52\n",
      "\n",
      "Step 1800\n",
      "Mean loss: 2.482846553325653\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 12.19\n",
      "Validation set perplexity: 11.28\n",
      "\n",
      "Step 1900\n",
      "Mean loss: 2.472129111289978\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 11.03\n",
      "Validation set perplexity: 11.27\n",
      "\n",
      "Step 2000\n",
      "Mean loss: 2.459598069190979\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 11.80\n",
      "================================================================================\n",
      "velash tha onintere t fhe fatoger as s aly eel by er ralasesedlarand an on f sa \n",
      "oif arf ard ded itrmulizthicolcit olietoerthe fonanend lot wre vers ansirevelery\n",
      "galintkthivepse tovelsbitecy ana cird mevamicivapandeere t l inor guss s tpa n d\n",
      "py baceava necbianliv llt e tiosed ilmotonan onen zeswarliwederok istne tednd it\n",
      "ianils ouse ax ascoral parstheght tama tunenm ife therthe gis ont honsto deshid \n",
      "================================================================================\n",
      "Validation set perplexity: 11.16\n",
      "\n",
      "Step 2100\n",
      "Mean loss: 2.4684371638298033\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 11.21\n",
      "Validation set perplexity: 11.37\n",
      "\n",
      "Step 2200\n",
      "Mean loss: 2.461016449928284\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 11.74\n",
      "Validation set perplexity: 11.28\n",
      "\n",
      "Step 2300\n",
      "Mean loss: 2.4587553668022157\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 12.08\n",
      "Validation set perplexity: 10.94\n",
      "\n",
      "Step 2400\n",
      "Mean loss: 2.438531169891357\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 11.56\n",
      "Validation set perplexity: 10.96\n",
      "\n",
      "Step 2500\n",
      "Mean loss: 2.4332669258117674\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 11.35\n",
      "Validation set perplexity: 10.70\n",
      "\n",
      "Step 2600\n",
      "Mean loss: 2.4372701454162597\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 11.40\n",
      "Validation set perplexity: 11.57\n",
      "\n",
      "Step 2700\n",
      "Mean loss: 2.4369079446792603\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 12.26\n",
      "Validation set perplexity: 11.11\n",
      "\n",
      "Step 2800\n",
      "Mean loss: 2.4328952074050902\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 12.08\n",
      "Validation set perplexity: 11.30\n",
      "\n",
      "Step 2900\n",
      "Mean loss: 2.4249692487716676\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 11.67\n",
      "Validation set perplexity: 10.97\n",
      "\n",
      "Step 3000\n",
      "Mean loss: 2.4440502452850343\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 12.42\n",
      "================================================================================\n",
      " athinemal fon lethe ll k ewrng siraclarlonglyirere fameoymame aucovet hrorumprm\n",
      "a s any cocokesey theerzlserasms cheteldisderoy is fes enshete temllbr by cs sep\n",
      "pes ca skediome yngid r laeahe botherr srod farorixenenge e of ilieben thero wok\n",
      "birlonitholastamumernes t echeeroulellera tiffede soe honeemome ce lisiniessly l\n",
      "sthegusicaf finsaromapitiv is c teals wefecaryitide thenithtbericoveofiandicaldi\n",
      "================================================================================\n",
      "Validation set perplexity: 11.19\n",
      "\n",
      "Step 3100\n",
      "Mean loss: 2.4224267482757567\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 11.10\n",
      "Validation set perplexity: 10.84\n",
      "\n",
      "Step 3200\n",
      "Mean loss: 2.424554965496063\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 11.80\n",
      "Validation set perplexity: 11.62\n",
      "\n",
      "Step 3300\n",
      "Mean loss: 2.4257901167869567\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 10.85\n",
      "Validation set perplexity: 11.04\n",
      "\n",
      "Step 3400\n",
      "Mean loss: 2.424575788974762\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 11.03\n",
      "Validation set perplexity: 10.94\n",
      "\n",
      "Step 3500\n",
      "Mean loss: 2.4065626096725463\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 10.98\n",
      "Validation set perplexity: 10.87\n",
      "\n",
      "Step 3600\n",
      "Mean loss: 2.4938808155059813\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 13.24\n",
      "Validation set perplexity: 12.36\n",
      "\n",
      "Step 3700\n",
      "Mean loss: 2.520900182723999\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 12.19\n",
      "Validation set perplexity: 12.47\n",
      "\n",
      "Step 3800\n",
      "Mean loss: 2.5031897711753843\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 12.41\n",
      "Validation set perplexity: 11.77\n",
      "\n",
      "Step 3900\n",
      "Mean loss: 2.499769687652588\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 13.60\n",
      "Validation set perplexity: 11.59\n",
      "\n",
      "Step 4000\n",
      "Mean loss: 2.4855579948425293\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 12.91\n",
      "================================================================================\n",
      "ire s jorerle arie gn ne anatono ch maughe cus th te etlt s d s t olat cion jran\n",
      " iob zait thun cok iou f s d coro m phadtwe giot th bem bris ones n lula f oa ba\n",
      "ce on go shimer bacly it tede ijin s d f am this w f f k al icie sthiviond mpr t\n",
      "w olan beregothemaf annli a n ha ave s blting s f ise be o ong n py lt mal n m t\n",
      "ze and ale beris ghs g aneptacivin mare fie omby es skt an s f ans a n on  orn l\n",
      "================================================================================\n",
      "Validation set perplexity: 12.08\n",
      "\n",
      "Step 4100\n",
      "Mean loss: 2.5047266626358033\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 12.60\n",
      "Validation set perplexity: 11.67\n",
      "\n",
      "Step 4200\n",
      "Mean loss: 2.4810266947746276\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 11.61\n",
      "Validation set perplexity: 11.92\n",
      "\n",
      "Step 4300\n",
      "Mean loss: 2.4660269665718078\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 11.40\n",
      "Validation set perplexity: 11.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4400\n",
      "Mean loss: 2.4666616106033326\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 11.97\n",
      "Validation set perplexity: 11.18\n",
      "\n",
      "Step 4500\n",
      "Mean loss: 2.458908097743988\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 12.02\n",
      "Validation set perplexity: 11.17\n",
      "\n",
      "Step 4600\n",
      "Mean loss: 2.4598174786567686\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 12.93\n",
      "Validation set perplexity: 12.09\n",
      "\n",
      "Step 4700\n",
      "Mean loss: 2.477580111026764\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 12.30\n",
      "Validation set perplexity: 11.73\n",
      "\n",
      "Step 4800\n",
      "Mean loss: 2.4469942355155947\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 11.01\n",
      "Validation set perplexity: 11.25\n",
      "\n",
      "Step 4900\n",
      "Mean loss: 2.4514136266708375\n",
      "Learning rate: 10.0\n",
      "Minibatch perplexity: 12.25\n",
      "Validation set perplexity: 11.58\n",
      "\n",
      "Step 5000\n",
      "Mean loss: 2.465315589904785\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 10.49\n",
      "================================================================================\n",
      "j arey t oneeiloa elamerimas oraonos ecssra mezingy nerosthe of med goms ris o b\n",
      "ilisee bal cs sie lored ad ve he fivatistmedsan ncomaveaupone fono iot chno mong\n",
      "olthowin riganeveuit sincand magherys sonicothym nituk oth fove onm t cine nindh\n",
      "ey s theananeriova jof ness ce ders tre zipear ce ttth t t ammm fid witsigafitis\n",
      "q wond han fe is kteca erapl led fon f chut font saraneion kondnan wk chaft lete\n",
      "================================================================================\n",
      "Validation set perplexity: 11.18\n",
      "\n",
      "Step 5100\n",
      "Mean loss: 2.4049847078323365\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 11.19\n",
      "Validation set perplexity: 10.95\n",
      "\n",
      "Step 5200\n",
      "Mean loss: 2.402708902359009\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 10.40\n",
      "Validation set perplexity: 10.92\n",
      "\n",
      "Step 5300\n",
      "Mean loss: 2.395962553024292\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 11.06\n",
      "Validation set perplexity: 10.94\n",
      "\n",
      "Step 5400\n",
      "Mean loss: 2.39757075548172\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 11.06\n",
      "Validation set perplexity: 10.99\n",
      "\n",
      "Step 5500\n",
      "Mean loss: 2.3933322048187256\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 10.97\n",
      "Validation set perplexity: 10.99\n",
      "\n",
      "Step 5600\n",
      "Mean loss: 2.390200021266937\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 11.04\n",
      "Validation set perplexity: 10.99\n",
      "\n",
      "Step 5700\n",
      "Mean loss: 2.412237639427185\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 11.93\n",
      "Validation set perplexity: 10.93\n",
      "\n",
      "Step 5800\n",
      "Mean loss: 2.3950533723831176\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 10.42\n",
      "Validation set perplexity: 10.88\n",
      "\n",
      "Step 5900\n",
      "Mean loss: 2.3937423777580262\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 10.51\n",
      "Validation set perplexity: 10.99\n",
      "\n",
      "Step 6000\n",
      "Mean loss: 2.3766245460510254\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 11.37\n",
      "================================================================================\n",
      "me serthe cos stwane tr nlaf s thed enmefee anan ats e difaw bof ds det pory an \n",
      "alat merolaund alil an levothean te trind and ad jof de clane s fory fig auapof \n",
      "hemepoonc wimuoier bris s ticr ce beranca raned ares vers gheacl fpigof hol logh\n",
      "aose ising dedrery tht pis o u faixeloke re hosend thiserintherebey as hinsv haw\n",
      "zipo win nore niserd iove ngper ton omo onsghryst a onghy texorit sive cst otht \n",
      "================================================================================\n",
      "Validation set perplexity: 10.94\n",
      "\n",
      "Step 6100\n",
      "Mean loss: 2.3890364241600035\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 10.58\n",
      "Validation set perplexity: 10.83\n",
      "\n",
      "Step 6200\n",
      "Mean loss: 2.399109387397766\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 11.35\n",
      "Validation set perplexity: 10.80\n",
      "\n",
      "Step 6300\n",
      "Mean loss: 2.3942711353302\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 10.28\n",
      "Validation set perplexity: 10.85\n",
      "\n",
      "Step 6400\n",
      "Mean loss: 2.392405831813812\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 11.46\n",
      "Validation set perplexity: 10.84\n",
      "\n",
      "Step 6500\n",
      "Mean loss: 2.3842460250854494\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 10.63\n",
      "Validation set perplexity: 10.82\n",
      "\n",
      "Step 6600\n",
      "Mean loss: 2.3814819478988647\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 10.71\n",
      "Validation set perplexity: 10.68\n",
      "\n",
      "Step 6700\n",
      "Mean loss: 2.3801812028884886\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 10.66\n",
      "Validation set perplexity: 10.68\n",
      "\n",
      "Step 6800\n",
      "Mean loss: 2.3820613265037536\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 11.00\n",
      "Validation set perplexity: 10.67\n",
      "\n",
      "Step 6900\n",
      "Mean loss: 2.398762788772583\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 11.10\n",
      "Validation set perplexity: 10.66\n",
      "\n",
      "Step 7000\n",
      "Mean loss: 2.3883478808403016\n",
      "Learning rate: 1.0\n",
      "Minibatch perplexity: 10.22\n",
      "================================================================================\n",
      " br tong tio d t bray de dis daugot oro wonaf tisgere tiroolteratherum jos inaom\n",
      "wacuro rate rris ite ns cas ter pluloeras e cr herstors o zeria onieiraris sco i\n",
      " of sinin b cas lom itheyiernllnotheo wes sun he teatenirdit merons ly tiay th t\n",
      "uo fowln sivark auase thof bre gind ous h ansulant thenicl ls dis zesewucatined \n",
      "t alsuscokn ice attitive ts ay ba sang rske t dess fiverinur loverbus con wher n\n",
      "================================================================================\n",
      "Validation set perplexity: 10.59\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    print('Initialized')\n",
    "    \n",
    "    mean_loss = 0\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "    \n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        \n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        \n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "        \n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            \n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('\\nStep {}'.format(step))\n",
    "            print('Mean loss: {}'.format(mean_loss))\n",
    "            print('Learning rate: {}'.format(lr))\n",
    "            \n",
    "            mean_loss = 0\n",
    "            \n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            \n",
    "            print('Minibatch perplexity: {:.2f}'.format(float(np.exp(logprob(predictions, labels)))))\n",
    "            \n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                \n",
    "                print('=' * 80)\n",
    "            \n",
    "                for _ in range(5):\n",
    "                    \n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    \n",
    "                    for _ in range(79):\n",
    "                        \n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                        \n",
    "                    print(sentence)\n",
    "                    \n",
    "                print('=' * 80)\n",
    "            \n",
    "            # Measure validation set perplexity.\n",
    "            \n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            \n",
    "            for _ in range(valid_size):\n",
    "                \n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            \n",
    "            print('Validation set perplexity: {:.2f}'.format(float(np.exp(valid_logprob / valid_size))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
